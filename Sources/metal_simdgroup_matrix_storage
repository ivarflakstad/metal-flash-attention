// -*- Metal -*-
//===-- metal_simdgroup_matrix_storage ------------------------------------===//
// Copyright (c) 2023 Philip Turner. See MIT LICENSE
//===----------------------------------------------------------------------===//


#ifndef __METAL_SIMDGROUP_MATRIX_STORAGE
#define __METAL_SIMDGROUP_MATRIX_STORAGE


// Contains C++ symbols accessible to a developer through automatic code
// completion in Xcode 14.2. Formatted with the same style as the Metal Standard
// Library for consistency with other Metal code.
#include <metal_stdlib>
#if defined(__HAVE_SIMDGROUP_MATRIX__)
#pragma METAL internals : enable

namespace bfloat16_impl {

  template <typename T>
  struct is_float_double {
    constexpr static constant bool value = false;
  };
  template <>
  struct is_float_double<half> {
    constexpr static constant bool value = true;
  };
  template <>
  struct is_float_double<float> {
    constexpr static constant bool value = true;
  };

  union float_raw {
    float f;
    uint32_t i;
  };

  METAL_FUNC uint32_t float_to_raw(float f) {
    float_raw r;
    r.f = f;
    return r.i;
  }

  METAL_FUNC float raw_to_float(uint32_t i) {
    float_raw r;
    r.i = i;
    return r.f;
  }

} /* namespace bfloat16_impl */

namespace metal
{
  
  struct bf16 {
    ushort raw;

    bf16(int raw_) : raw(raw_) {}
    bf16(ushort raw_) : raw(raw_) {}

    bf16() = default;
    METAL_FUNC bf16(float f);

    METAL_FUNC operator float() const;
    METAL_FUNC operator ushort() const;
  };

  bf16::bf16(float f) {
    raw = bfloat16_impl::float_to_raw(f) >> 16; // RTZ
  }

  METAL_FUNC bf16::operator float() const {
    return bfloat16_impl::raw_to_float(raw << 16);
  }

  METAL_FUNC bf16::operator ushort() const {
    return raw;
  }
  
  template <typename T, typename M = float>
  struct simdgroup_matrix_storage {
    typedef vec<T, 64> storage_type;
    typedef vec<M, 64> multiply_type;
    
    storage_type t;
    
    METAL_FUNC thread vec<T, 2>* thread_elements() thread {
      return reinterpret_cast<thread vec<T, 2>*>(&t);
    }
    
    METAL_FUNC simdgroup_matrix_storage() thread = default;
    
    METAL_FUNC simdgroup_matrix_storage(vec<T, 2> thread_elements) thread {
      *(this->thread_elements()) = thread_elements;
    }
    
    METAL_FUNC static ushort2 offset(ushort thread_index_in_simdgroup) {
      // https://patents.google.com/patent/US11256518B2
      ushort lane_id = thread_index_in_simdgroup;
      ushort quad_id = lane_id / 4;
      
      constexpr ushort QUADRANT_SPAN_M = 4;
      constexpr ushort THREADS_PER_QUADRANT = 8;
      ushort M_floor_of_quadrant = (quad_id / 4) * QUADRANT_SPAN_M;
      ushort M_in_quadrant = (lane_id / 2) % (THREADS_PER_QUADRANT / 2);
      ushort M_in_simd = M_floor_of_quadrant + M_in_quadrant;
      
      ushort N_floor_of_quadrant = (quad_id & 2) * 2; // 0 or 4
      ushort N_in_quadrant = (lane_id % 2) * 2; // 0 or 2
      ushort N_in_simd = N_floor_of_quadrant + N_in_quadrant;
      
      return ushort2(N_in_simd, M_in_simd);
    }
    
    METAL_FUNC static device T* apply_offset(device T *src, uint elements_per_row, uint2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        return src + ulong(matrix_origin.x * elements_per_row) + matrix_origin.y;
      } else {
        return src + ulong(matrix_origin.y * elements_per_row) + matrix_origin.x;
      }
    }
    
    METAL_FUNC static threadgroup T* apply_offset(threadgroup T *src, ushort elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        return src + matrix_origin.x * elements_per_row + matrix_origin.y;
      } else {
        return src + matrix_origin.y * elements_per_row + matrix_origin.x;
      }
    }
    
    // WARNING: All load and store functions assume the X dimension is divisible by 2.
    
    METAL_FUNC void load(const device T *src, uint elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        *(thread_elements()) = vec<T, 2>(src[ulong(matrix_origin.x * elements_per_row) + matrix_origin.y], src[ulong((matrix_origin.x + 1) * elements_per_row) + matrix_origin.y]);
      } else {
        *(thread_elements()) = *reinterpret_cast<const device vec<T, 2>*>(src + ulong(matrix_origin.y * elements_per_row) + matrix_origin.x);
      }
    }
    
    METAL_FUNC void load(const threadgroup T *src, ushort elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        *(thread_elements()) = vec<T, 2>(src[matrix_origin.x * elements_per_row + matrix_origin.y], src[(matrix_origin.x + 1) * elements_per_row + matrix_origin.y]);
      } else {
        *(thread_elements()) = *reinterpret_cast<const threadgroup vec<T, 2>*>(src + matrix_origin.y * elements_per_row + matrix_origin.x);
      }
    }
    
    METAL_FUNC void load_first(const device T *src, ushort elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        thread_elements()[0][0] = src[matrix_origin.x * elements_per_row + matrix_origin.y];
      } else {
        thread_elements()[0][0] = src[matrix_origin.y * elements_per_row + matrix_origin.x];
      }
    }
    
    METAL_FUNC void load_second(const device T *src, ushort elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        thread_elements()[0][1] = src[matrix_origin.x * elements_per_row + matrix_origin.y];
      } else {
        thread_elements()[0][1] = src[matrix_origin.y * elements_per_row + matrix_origin.x];
      }
    }
    
    METAL_FUNC void store(device T *dst, uint elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        dst[ulong(matrix_origin.x * elements_per_row) + matrix_origin.y] = thread_elements()[0][0];
        dst[ulong((matrix_origin.x + 1) * elements_per_row) + matrix_origin.y] = thread_elements()[0][1];
      } else {
        *reinterpret_cast<device vec<T, 2>*>(dst + matrix_origin.y * elements_per_row + matrix_origin.x) = *(thread_elements());
      }
    }
    
    METAL_FUNC void store_first(device T *dst, uint elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        dst[ulong(matrix_origin.x * elements_per_row) + matrix_origin.y] = thread_elements()[0][0];
      } else {
        dst[matrix_origin.y * elements_per_row + matrix_origin.x] = thread_elements()[0][0];
      }
    }
    
    METAL_FUNC void store_second(device T *dst, uint elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        dst[ulong(matrix_origin.x * elements_per_row) + matrix_origin.y] = thread_elements()[0][1];
      } else {
        dst[matrix_origin.y * elements_per_row + matrix_origin.x] = thread_elements()[0][1];
      }
    }
    
    METAL_FUNC void store(threadgroup T *dst, ushort elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        dst[matrix_origin.x * elements_per_row + matrix_origin.y] = thread_elements()[0][0];
        dst[(matrix_origin.x + 1) * elements_per_row + matrix_origin.y] = thread_elements()[0][1];
      } else {
        *reinterpret_cast<threadgroup vec<T, 2>*>(dst + matrix_origin.y * elements_per_row + matrix_origin.x) = *(thread_elements());
      }
    }
    
    template <typename U, typename V>
    METAL_FUNC void multiply(simdgroup_matrix_storage<U> a, simdgroup_matrix_storage<V> b, bool accumulate = true) {
      if (!accumulate) {
        *(thread_elements()) = vec<T, 2>(0);
      }
      t = __metal_simdgroup_matrix_8x8_multiply_accumulate(a.t, b.t, t, typename simdgroup_matrix_storage<T>::storage_type());
    }
    
//    constexpr METAL_FUNC thread ushort2* float_to_bfloat_bits(vec<float, 64> x) thread {
//      
//      thread ushort2 a = ushort2 { bf16(x[0]), bf16(x[1]) };
//      return reinterpret_cast<thread ushort2*>(&a);
//    }
    
    constexpr METAL_FUNC vec<ushort, 64> f_to_bf(vec<float, 64> x) {
        vec<ushort, 64> result = {};
        #pragma clang loop unroll(full)
        for (uint i = 0; i < 64; i += 1) {
          result[i] = bf16(x[i]);
        }
        return result;
    }
    
    constexpr METAL_FUNC vec<float, 64> bf_to_f(vec<ushort, 64> x) {
        vec<float, 64> result = {};
        #pragma clang loop unroll(full)
        for (uint i = 0; i < 64; i += 1) {
          result[i] = bf16(x[i]);
        }
        return result;
    }
    
    
    METAL_FUNC void multiply(simdgroup_matrix_storage<ushort> a, simdgroup_matrix_storage<ushort> b, bool accumulate = true) {
      multiply_bf16_alt1(a, b, accumulate);
    }
    
    
    METAL_FUNC void multiply_bf16_alt2(simdgroup_matrix_storage<ushort> a, simdgroup_matrix_storage<ushort> b, bool accumulate = true) {
      if (!accumulate) {
        *(thread_elements()) = vec<T, 2>(0);
      }
      
      simdgroup_matrix_storage<float> temp_a;
      *(temp_a.thread_elements()) = load_bfloat_input(a.thread_elements());
      simdgroup_matrix_storage<float> temp_b;
      *(temp_b.thread_elements()) = load_bfloat_input(b.thread_elements());
      
      simdgroup_matrix_storage<float> temp_result;
      *(temp_result.thread_elements()) = load_bfloat_input(thread_elements());
      temp_result.t = __metal_simdgroup_matrix_8x8_multiply_accumulate(
                                                                   temp_a.t,
                                                                   temp_b.t,
                                                                   temp_result.t,
                                                                   typename simdgroup_matrix_storage<M>::storage_type()
                                                                   );
      store_bfloat_result(temp_result.thread_elements());
    }
    
    
    METAL_FUNC void multiply_bf16_alt1(simdgroup_matrix_storage<ushort> a, simdgroup_matrix_storage<ushort> b, bool accumulate = true) {
      if (!accumulate) {
        *(thread_elements()) = vec<T, 2>(0);
      }
      
      t = f_to_bf(__metal_simdgroup_matrix_8x8_multiply_accumulate(
                                                                   bf_to_f(a.t),
                                                                   bf_to_f(b.t),
                                                                   bf_to_f(t),
                                                                   typename simdgroup_matrix_storage<M>::storage_type()
                                                                   ));
    }
    
    
    METAL_FUNC vec<float, 2> load_bfloat_input(thread vec<ushort, 2> *a) thread {
      return vec<float, 2>( bf16((*a)[0]), bf16((*a)[1]) );
    }
    
    METAL_FUNC void store_bfloat_result(thread vec<float, 2> *a) thread {
      *(thread_elements()) = vec<ushort, 2>( bf16((*a)[0]), bf16((*a)[1]) );
    }
    
    // 'bfloat' is 'float' with the lower 16 bits set to garbage (BF15).
    
    METAL_FUNC thread ushort4* thread_elements_bfloat() thread {
      thread float2* elements = thread_elements();
      return reinterpret_cast<thread ushort4*>(elements);
    }
    
    METAL_FUNC simdgroup_matrix_storage<float> unpack_bfloat() thread {
      ushort4 output;
      thread ushort2& elements = thread_elements();
      output.y = elements[0];
      output.w = elements[1];
      return simdgroup_matrix_storage(as_type<float2>(output));
    }
    
    METAL_FUNC simdgroup_matrix_storage<ushort> pack_bfloat() thread {
      thread ushort4* elements = thread_elements_bfloat();
      return simdgroup_matrix_storage(ushort2(elements->y, elements->w));
    }
    
    METAL_FUNC void load_bfloat(const threadgroup ushort *src, ushort elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        thread_elements_bfloat()->y = src[matrix_origin.x * elements_per_row + matrix_origin.y];
        thread_elements_bfloat()->w = src[(matrix_origin.x + 1) * elements_per_row + matrix_origin.y];
      } else {
        thread_elements_bfloat()->zw = *reinterpret_cast<const threadgroup ushort2*>(src + matrix_origin.y * elements_per_row + matrix_origin.x);
        thread_elements_bfloat()->y = thread_elements_bfloat()->z;
      }
    }
    
    METAL_FUNC void store_bfloat(threadgroup ushort *dst, ushort elements_per_row, ushort2 matrix_origin, bool transpose_matrix = false) {
      if (transpose_matrix) {
        dst[matrix_origin.x * elements_per_row + matrix_origin.y] = *(thread_elements_bfloat()).y;
        dst[(matrix_origin.x + 1) * elements_per_row + matrix_origin.y] = *(thread_elements_bfloat()).w;
      } else {
        *(thread_elements_bfloat()).z = *(thread_elements_bfloat()).y;
        *reinterpret_cast<threadgroup vec<T, 2>*>(dst + matrix_origin.y * elements_per_row + matrix_origin.x) = *(thread_elements_bfloat()).zw;
      }
    }
  };
} // namespace metal
#pragma METAL internals : disable
#endif

#endif // __METAL_SIMDGROUP_MATRIX_STORAGE
